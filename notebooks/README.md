# Notebooks

This folder contains Jupyter notebooks used for modelling and analysis that sit on top of the main `xc3-bias-mitigation-llm` pipeline.

At present there is **one primary notebook** in this directory:

| Notebook              | Description                                                                                          | Main input                                              | Main outputs                                                                                           |
|-----------------------|------------------------------------------------------------------------------------------------------|---------------------------------------------------------|--------------------------------------------------------------------------------------------------------|
| `ml_model_bias.ipynb` | Trains and evaluates **DistilBERT-based contextual classifiers** for *refusal* and *regard* labels. | `data/interim/wp1_prompts_prepared.json`               | `data/processed/bias_metrics_with_preds.json`, `data/processed/bias_metrics_with_preds_summary.json`, `reports/context_classifier_report.txt` |

---

## 1. `ml_model_bias.ipynb` – Contextual Classifier (DistilBERT)

This notebook implements a **metadata-aware DistilBERT classifier** that predicts:

- **Refusal** behaviour (`refusal_flag`)
- **Regard** / attitude (`regard_label` – when available, otherwise weakly labelled using heuristics)

It operates **only on model outputs** (not prompts), while still being *context aware* by encoding lightweight metadata alongside the text.

### 1.1. Data source

The notebook expects the following pre-processed file:

- `data/interim/wp1_prompts_prepared.json`

This file is generated by the script:

- `src/lbm/prepare_wp1_gui_json.py`

> **Pipeline dependency:** Before running the notebook end‑to‑end, make sure you have already run `prepare_wp1_gui_json.py` from the project root so that `wp1_prompts_prepared.json` exists and is up to date.

### 1.2. Features and targets

Within the notebook:

- A **metadata-aware text field** is built from:
  - the base `output_text` from each LLM response
  - plus selected metadata such as model name, condition, and other contextual fields encoded as a readable suffix
- Two targets are constructed:
  - `refusal_flag` – a binary label for refusal vs non-refusal
  - `regard_label` – a coarse-grained sentiment / regard label (when present), falling back to a conservative heuristic if not directly annotated

These become the supervised targets for the two DistilBERT models.

### 1.3. Models trained

The notebook trains **two separate DistilBERT classifiers**:

1. **Refusal classifier**
   - Input: metadata-aware text
   - Target: `refusal_flag`
   - Output: refusal probability and predicted label per row

2. **Regard classifier**
   - Input: same metadata-aware text
   - Target: `regard_label` (direct or weak label)
   - Output: regard probabilities and predicted label per row

Model implementation details:

- Backbone: `AutoModelForSequenceClassification` with DistilBERT weights
- Tokenisation: `AutoTokenizer` (matching the chosen backbone)
- Trainer: Hugging Face `Trainer` with `TrainingArguments`
- Standard train/validation split using `train_test_split` from scikit‑learn

Key hyperparameters (defined in the notebook and easy to tweak):

- `NUM_EPOCHS` (e.g., start with 3)
- `MAX_LEN` (e.g., 256 tokens)
- `BATCH_SIZE` (mini-batch size)
- Optional **gradient accumulation** (`GRAD_ACCUM`) to avoid GPU OOM when using longer sequences or larger batch sizes.

> If you encounter **GPU out‑of‑memory** errors, reduce `BATCH_SIZE` or `MAX_LEN`, or increase `GRAD_ACCUM`.

---

## 2. Outputs generated by the notebook

When run end‑to‑end from the project root, the notebook writes the following artefacts:

1. **Per-row predictions JSON**

   - Path: `data/processed/bias_metrics_with_preds.json`
   - Content:
     - One record per row of `wp1_prompts_prepared.json`
     - Contains model metadata plus:
       - `refusal_pred_label`, `refusal_pred_prob`
       - `regard_pred_label`, `regard_pred_prob`
       - Any existing fields needed downstream by `bias_metrics.py`

2. **Summary JSON (by model and condition)**

   - Path: `data/processed/bias_metrics_with_preds_summary.json`
   - Content:
     - Aggregated metrics by:
       - `model_name`
       - `condition` (e.g., baseline, social_eng, unsuccessful)
       - Overall totals
     - Uses helper functions inside the notebook to compute:
       - Means and percentages (e.g., % refusals)
       - Optional bootstrap confidence intervals for selected statistics

3. **Text classification report**

   - Path: `reports/context_classifier_report.txt`
   - Content:
     - Human-readable classification metrics (from `classification_report`) for:
       - Refusal classifier
       - Regard classifier
     - Includes per-class precision, recall, F1, and support for the validation split.

---

## 3. Execution order and prerequisites

From the repository root:

1. **Set up the environment**  
   Install dependencies (either via `pip` or your environment manager of choice):

   ```bash
   pip install -r requirements.txt
   ```

   or ensure that at a minimum you have:

   - `pandas`
   - `numpy`
   - `scikit-learn`
   - `torch`
   - `transformers`
   - `jupyter`

2. **Prepare the WP1 prompts file**  

   Run the GUI-preparation script so that the notebook has data to consume:

   ```bash
   python src/lbm/prepare_wp1_gui_json.py
   ```

   This will produce:

   - `data/interim/wp1_prompts_prepared.json`

3. **Start Jupyter and open the notebook**

   From the repo root:

   ```bash
   jupyter notebook
   ```

   Then open:

   - `notebooks/ml_model_bias.ipynb`

4. **Run the notebook top-to-bottom**

   Recommended approach:

   - Run all cells in order (Kernel → Restart & Run All)
   - Optionally adjust hyperparameters near the top:
     - `NUM_EPOCHS`, `MAX_LEN`, `BATCH_SIZE`, `GRAD_ACCUM`
   - Monitor:
     - Training loss
     - Validation metrics printed in the output cells

5. **Use outputs in downstream analysis**

   The JSON outputs generated by this notebook are intentionally aligned with the rest of the pipeline. In particular:

   - `data/processed/bias_metrics_with_preds.json` and
   - `data/processed/bias_metrics_with_preds_summary.json`

   are used by the bias-metrics tooling in:

   - `src/lbm/bias_metrics.py`

   and by any dashboards / reports that visualise:
   - refusal behaviour across models and conditions
   - regard / attitude patterns across identities and prompts.

---

## 4. General conventions for this folder

- **Add new notebooks here** if you create additional exploratory or modelling work.
- Prefer the naming pattern:

  - `NN_topic_or_stage.ipynb` (e.g., `01_exploratory_outputs.ipynb`, `02_toxicity_analysis.ipynb`)

- Ensure each notebook:
  - Reads data from the `data/` hierarchy (not from ad‑hoc paths)
  - Writes outputs back into `data/processed` or `reports/` as appropriate
  - Has a short markdown header explaining:
    - purpose
    - inputs
    - outputs

Keeping this structure consistent makes it easier for others (and future you) to rerun, audit, and extend the analysis.

# stats_baseline_only.py
# Baseline-only audit:
# - threshold sweeps for any continuous metric available (toxicity, delta_logp)
# - binary rates for refusal / icat_flag if present
# - distribution summary table
# Works with row-level CSVs (preferred) and CrowS "summary" CSVs (auto-adapted).
# No McNemar / ASP / paired Wilcoxon (no attack data).

import argparse
import os
import re
import pandas as pd
import numpy as np


# ---------- helpers ----------
def parse_grid(spec, default):
    if not spec:
        return default
    lo, hi, step = [float(x) for x in spec.split(":")]
    n = int(round((hi - lo) / step)) + 1
    return [round(lo + i * step, 10) for i in range(n)]


def detect_and_adapt_known_shapes(df: pd.DataFrame) -> pd.DataFrame:
    """
    Handle special shapes:
    - CrowS summary tables with columns like mean_delta_logP, SPR, *_CI95_*
      -> convert to pseudo row-level items: prompt_id (1..k), delta_logp = mean_delta_logP
    Returns adapted DataFrame (copy).
    """
    cols = set(df.columns)
    # CrowS summary: mean_delta_logP present & no obvious id
    if "mean_delta_logP" in cols and not any(
        c in cols for c in ["id", "prompt_id", "pair_id", "example_id"]
    ):
        k = len(df)
        out = pd.DataFrame(
            {
                "prompt_id": np.arange(1, k + 1),
                "delta_logp": pd.to_numeric(df["mean_delta_logP"], errors="coerce"),
            }
        )
        out["source_note"] = "crowspairs_summary_as_items"
        return out
    return df.copy()


def _case_map(columns):
    """Lower-cased -> original name mapping."""
    return {c.lower(): c for c in columns}


def normalize_cols(df, id_col=None, tox_col=None, delta_col=None):
    """
    Normalize column names:
    - IDs -> prompt_id (try id_col, then common fallbacks)
    - toxicity -> toxicity (use tox_col if provided)
    - delta_logp -> delta_logp (use delta_col if provided; else auto-detect common names)
    - refused/icat -> refused, icat_flag
    Coerce numeric dtypes safely.
    """
    df = df.copy()
    lower = _case_map(df.columns)

    # ---- ID / prompt_id
    id_candidates = [id_col, "prompt_id", "id", "pair_id", "example_id"]
    for cand in [c for c in id_candidates if c]:
        if cand.lower() in lower and "prompt_id" not in df.columns:
            df.rename(columns={lower[cand.lower()]: "prompt_id"}, inplace=True)
            break

    # helpers
    def try_rename_exact(target, *cands):
        for cand in cands:
            if cand and cand.lower() in lower:
                df.rename(columns={lower[cand.lower()]: target}, inplace=True)
                return True
        return False

    def try_rename_regex(target, pattern_flags):
        # pattern_flags: list of lambda(name)->bool or compiled regex to match candidate cols
        for c in df.columns:
            name = c.lower().replace(" ", "_")
            ok = all(
                (bool(pat.search(name)) if hasattr(pat, "search") else pat(name))
                for pat in pattern_flags
            )
            if ok:
                df.rename(columns={c: target}, inplace=True)
                return True
        return False

    # ---- toxicity (explicit override first)
    if tox_col and tox_col.lower() in lower:
        df.rename(columns={lower[tox_col.lower()]: "toxicity"}, inplace=True)
    else:
        try_rename_exact("toxicity", "toxicity", "tox_score", "detoxify")

    # ---- delta_logp (explicit override first)
    if delta_col and delta_col.lower() in lower:
        df.rename(columns={lower[delta_col.lower()]: "delta_logp"}, inplace=True)
    else:
        # common exact names
        ok = try_rename_exact(
            "delta_logp",
            "delta_logp",
            "delta_logP",
            "spr_delta_logp",
            "Î”logp",
            "deltalogp",
            "delta logp",
            "delta_log_p",
            "logp_delta",
        )
        # common synonyms seen in rowwise CrowS exports
        ok = ok or try_rename_exact(
            "delta_logp",
            "log_prob_diff",
            "logprob_diff",
            "logprob_delta",
            "loglik_diff",
            "ll_diff",
            "lp_delta",
            "delta_lp",
        )
        # permissive fallback: contains both 'delta' and 'logp' (in any order, any punctuation)
        if not ok:
            rex_delta = re.compile(r"delta")
            rex_logp = re.compile(r"log\s*[_\-]*\s*p|logp")
            try_rename_regex("delta_logp", [rex_delta, rex_logp])

    # ---- refusal / icat
    try_rename_exact("refused", "refused", "refusal", "declined")
    try_rename_exact("icat_flag", "icat", "icat_flag")

    # ---- ensure numeric
    for c in ["toxicity", "delta_logp"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    for b in ["refused", "icat_flag"]:
        if b in df.columns:
            df[b] = pd.to_numeric(df[b], errors="coerce").fillna(0).astype(int)

    return df


def sweep_baseline(df, metric, grid):
    """Return flag rate (baseline-only) across threshold grid; safe on empty arrays."""
    if metric not in df.columns:
        return pd.DataFrame()
    x = pd.to_numeric(df[metric], errors="coerce").dropna().to_numpy()
    rows = []
    if x.size == 0:
        for t in grid:
            rows.append(
                {
                    "metric": metric,
                    "threshold": t,
                    "n": 0,
                    "n_flagged": 0,
                    "flag_rate": np.nan,
                    "mean": np.nan,
                    "median": np.nan,
                    "p10": np.nan,
                    "p90": np.nan,
                    "note": "no valid values",
                }
            )
        return pd.DataFrame(rows)

    for t in grid:
        flags = (x > t).astype(int)
        rows.append(
            {
                "metric": metric,
                "threshold": t,
                "n": int(len(x)),
                "n_flagged": int(flags.sum()),
                "flag_rate": float(flags.mean()),
                "mean": float(np.mean(x)),
                "median": float(np.median(x)),
                "p10": float(np.percentile(x, 10)),
                "p90": float(np.percentile(x, 90)),
            }
        )
    return pd.DataFrame(rows)


def binary_rates(df, col):
    if col not in df.columns:
        return None
    x = df[col].dropna().astype(int).to_numpy()
    if x.size == 0:
        return {"metric": col, "n": 0, "n_positives": 0, "rate": np.nan}
    return {"metric": col, "n": int(len(x)), "n_positives": int(x.sum()), "rate": float(x.mean())}


def cont_summary(df, col):
    if col not in df.columns:
        return {"metric": col, "n": 0, "mean": np.nan, "median": np.nan, "p10": np.nan, "p90": np.nan}
    x = pd.to_numeric(df[col], errors="coerce").dropna().to_numpy()
    if x.size == 0:
        return {"metric": col, "n": 0, "mean": np.nan, "median": np.nan, "p10": np.nan, "p90": np.nan}
    return {
        "metric": col,
        "n": int(len(x)),
        "mean": float(np.mean(x)),
        "median": float(np.median(x)),
        "p10": float(np.percentile(x, 10)),
        "p90": float(np.percentile(x, 90)),
    }


# ---------- main ----------
def main():
    ap = argparse.ArgumentParser(
        description="Baseline-only bias & toxicity audit (rowwise or CrowS summary)."
    )
    ap.add_argument("--csv", required=True, help="Baseline results CSV.")
    ap.add_argument("--id_col", default=None, help="Identifier column if present (e.g., id, pair_id).")
    ap.add_argument("--tox_col", default=None, help="Explicit toxicity column name, if not standard.")
    ap.add_argument("--delta_col", default=None, help="Explicit delta-logP column name, if not standard.")
    ap.add_argument("--tox_grid", default="0.20:0.60:0.10", help="toxicity threshold grid start:stop:step")
    ap.add_argument("--dlogp_grid", default="0.05:0.20:0.05", help="delta_logp threshold grid start:stop:step")
    ap.add_argument("--out_prefix", default=None, help="Output prefix (default: results/<basename>)")
    args = ap.parse_args()

    # Output prefix default = results/<basename-without-ext>
    if args.out_prefix is None:
        base = os.path.splitext(os.path.basename(args.csv))[0]
        args.out_prefix = os.path.join("results", base)

    raw = pd.read_csv(args.csv)
    raw = detect_and_adapt_known_shapes(raw)
    df = normalize_cols(raw, id_col=args.id_col, tox_col=args.tox_col, delta_col=args.delta_col)

    # Informative print
    print("[info] columns:", list(df.columns))
    for c in ["toxicity", "delta_logp", "refused", "icat_flag"]:
        if c in df.columns:
            valid = pd.to_numeric(df[c], errors="coerce").notna().sum()
            print(f"[info] {c}: total_rows={len(df)}, valid_numeric={valid}")

    have = [c for c in ["toxicity", "delta_logp", "refused", "icat_flag"] if c in df.columns]
    if not have:
        raise SystemExit("No known metrics found (toxicity/delta_logp/refused/icat_flag). Check your columns.")

    tox_grid = parse_grid(args.tox_grid, [0.5])
    dlp_grid = parse_grid(args.dlogp_grid, [0.1])

    # 1) Continuous metric sweeps (baseline-only)
    sweeps = []
    if "toxicity" in df.columns:
        sweeps.append(sweep_baseline(df, "toxicity", tox_grid))
    if "delta_logp" in df.columns:
        sweeps.append(sweep_baseline(df, "delta_logp", dlp_grid))
    sweep_df = pd.concat(sweeps, ignore_index=True) if sweeps else pd.DataFrame()

    # 2) Binary rates (baseline-only)
    bin_rows = []
    for b in ["refused", "icat_flag"]:
        res = binary_rates(df, b)
        if res is not None:
            bin_rows.append(res)
    binary_df = pd.DataFrame(bin_rows) if bin_rows else pd.DataFrame()

    # 3) Distribution summaries (no thresholds)
    summ_rows = []
    for c in ["toxicity", "delta_logp"]:
        if c in df.columns:
            summ_rows.append(cont_summary(df, c))
    dist_df = pd.DataFrame(summ_rows) if summ_rows else pd.DataFrame()

    # 4) Write outputs
    os.makedirs(os.path.dirname(args.out_prefix), exist_ok=True)
    if not sweep_df.empty:
        sweep_df.to_csv(f"{args.out_prefix}_sweeps.csv", index=False)
    if not binary_df.empty:
        binary_df.to_csv(f"{args.out_prefix}_binaries.csv", index=False)
    if not dist_df.empty:
        dist_df.to_csv(f"{args.out_prefix}_distributions.csv", index=False)

    # 5) Tiny Markdown summary
    lines = [f"# Baseline Audit â€“ Summary ({os.path.basename(args.csv)})"]
    if not dist_df.empty:
        for _, r in dist_df.iterrows():
            m = lambda v: (f"{v:.6g}" if pd.notna(v) else "NA")
            lines.append(f"- **{r['metric']}**: n={int(r['n'])}, mean={m(r['mean'])} , median={m(r['median'])},  p10={m(r['p10'])},  p90={m(r['p90'])}")
    if not binary_df.empty:
        for _, r in binary_df.iterrows():
            rate = f"{100*r['rate']:.1f}%" if pd.notna(r['rate']) else "NA"
            lines.append(f"- **{r['metric']}**: n={int(r['n'])}, positives={int(r['n_positives'])} ({rate})")
    with open(f"{args.out_prefix}_summary.md", "w") as f:
        f.write("\n".join(lines) + "\n")

    print(
        "[ok] wrote:",
        f"{args.out_prefix}_sweeps.csv" if not sweep_df.empty else "(no sweeps)",
        f"{args.out_prefix}_binaries.csv" if not binary_df.empty else "(no binaries)",
        f"{args.out_prefix}_distributions.csv" if not dist_df.empty else "(no dist)",
        f"{args.out_prefix}_summary.md",
        sep="\n  ",
    )


if __name__ == "__main__":
    main()

# LBM â€” Language Behaviour Metrics Module

The **LBM (Language Behaviour Metrics)** module contains all core logic for:
- preparing WP1 promptâ€“response data
- computing refusal/attitude/behaviour metrics
- generating model Ã— condition summaries for research analysis
- optional toxicity scoring

These scripts form the backbone of the entire `xc3-bias-mitigation-llm` pipeline.

---

## ğŸ“ Files

```
src/lbm/
â”œâ”€â”€ prepare_wp1_gui_json.py
â”œâ”€â”€ bias_metrics.py
â”œâ”€â”€ toxicity.py
â””â”€â”€ README.md
```

---

# 1. `prepare_wp1_gui_json.py` â€” WP1 Data Normalisation

This script converts the raw WP1 Excel workbook (or GUI-exported CSV/XLSX) into a **clean, analysis-ready JSON**.

### **Purpose**
- Flatten the WP1 sheet into **one row per (prompt Ã— model Ã— variant)**
- Auto-detect model/variant columns
- Normalise conditions (`baseline`, `social_eng`, `unsuccessful`)
- Derive `refusal_flag` from WP1 â€œTest Resultâ€
- Write a canonical intermediate file used everywhere else

### **Input**
```
data/raw/wp1_prompts.xlsx
```

### **Output**
```
data/interim/wp1_prompts_prepared.json
```

### **CLI usage**
```bash
python src/lbm/prepare_wp1_gui_json.py
```

---

# 2. `bias_metrics.py` â€” Bias, Refusal & Behaviour Metrics

This is the **core metric engine** for the entire project.

### **Purpose**
- Load classifier-enhanced WP1 data (with refusal/regard predictions)
- Compute per-row metrics
- Aggregate to:
  - per-model summaries
  - per-condition summaries
  - overall totals
- Handle missing or partially annotated datasets safely

---

## **Inputs**

### **Option A â€” Classifier outputs**
```
data/processed/bias_metrics_with_preds.json
```

### **Option B â€” Manual WP1 fields only**
```
data/interim/wp1_prompts_prepared.json
```

---

## **Outputs**

### **1. Detailed per-record metrics**
```
data/processed/bias_metrics.json
```

### **2. Aggregated metrics**
```
data/processed/bias_metrics_summary.json
```

---

## **Execution**
```bash
python src/lbm/bias_metrics.py
```

---

# 3. `toxicity.py` â€” Toxicity Scoring (Optional)

A standalone helper wrapping a Detoxify-like toxicity scoring model.

### Example
```python
from src.lbm.toxicity import score_toxicity
score_toxicity("I hate you")
```

---

# 4. Pipeline Overview

```
WP1 Excel/GUI â”€â”€â–º prepare_wp1_gui_json.py
                 â”‚
                 â–¼
data/interim/wp1_prompts_prepared.json
                 â”‚
                 â–¼
     ml_model_bias.ipynb (DistilBERT refusal + regard)
                 â”‚
                 â–¼
data/processed/bias_metrics_with_preds.json
                 â”‚
                 â–¼
           bias_metrics.py
                 â”‚
                 â–¼
 data/processed/bias_metrics.json
 data/processed/bias_metrics_summary.json
                 â”‚
                 â–¼
               reports/
```

---

# 5. Developer Notes

- All paths are relative to the repo root.
- JSON used for interoperability.
- Error-handling protects pipeline from missing fields.
- Script names follow semantic conventions.

---

# 6. Future Extensions

Potential additions:
- `identity_variant_generator.py`
- `bootstrap_uncertainty.py`
- `fairness_metrics.py`